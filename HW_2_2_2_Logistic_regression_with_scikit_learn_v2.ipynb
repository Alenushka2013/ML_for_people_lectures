{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhoIXFHNP0LjKmVXuOxjW9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alenushka2013/ML_for_people_lectures/blob/main/HW_2_2_2_Logistic_regression_with_scikit_learn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Запит до Gemini 1"
      ],
      "metadata": {
        "id": "LK_Pmc7t_2C9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Потрібно визначити цільову змінну Exited за допомогою логістичної регресії\n",
        "\n",
        "Пропусків в даних немає\n",
        "\n",
        "На що потрібно звернути увагу при препроцесингу даних?\n",
        "\n",
        "які параметри доцільно використати при кодуванні і масштабуванні ознак?\n",
        "\n",
        "Як оптимально налаштувати модель логістичної регресії для цієї задачі?"
      ],
      "metadata": {
        "id": "xZZrVBJw_7rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Привіт! Чудова задача для логістичної регресії. Вона ідеально підходить для прогнозування бінарної цільової змінної, такої як `Exited` (відтік клієнтів).\n",
        "\n",
        "Ось на що варто звернути увагу при препроцесингу даних, які параметри використати для кодування та масштабування, а також як оптимально налаштувати модель логістичної регресії.\n",
        "\n",
        "---\n",
        "\n",
        "## Препроцесинг даних: На що звернути увагу\n",
        "\n",
        "Перед тим як подавати дані моделі логістичної регресії, важливо правильно їх підготувати.\n",
        "\n",
        "1.  **Видалення непотрібних стовпців:**\n",
        "    * **ID:** Цей стовпець є просто унікальним ідентифікатором рядка і не несе жодної інформації для моделі, тому його слід видалити.\n",
        "    * **Customer ID:** Ви вже зазначили, що його потрібно видалити через дублікати та відсутність прогностичної цінності. Абсолютно вірно.\n",
        "    * **Surname:** Прізвище клієнта зазвичай не є прогностичною ознакою для відтоку і може створювати зайвий шум або навіть призвести до витоку даних, якщо там є якась прихована інформація. Краще його видалити.\n",
        "\n",
        "2.  **Обробка категоріальних ознак:**\n",
        "    * Логістична регресія працює з числовими даними. Категоріальні ознаки, такі як `Geography` (Країна) та `Gender` (Стать), потрібно перетворити на числові.\n",
        "\n",
        "3.  **Масштабування числових ознак:**\n",
        "    * Логістична регресія, як і багато інших алгоритмів, чутлива до масштабу числових ознак. Ознаки з більшими значеннями можуть домінувати над ознаками з меншими значеннями, що може призвести до менш ефективного навчання моделі. Масштабування вирівнює діапазони значень ознак.\n",
        "\n",
        "4.  **Визначення цільової змінної:**\n",
        "    * `Exited` (`1` = залишив банк, `0` = не залишив) - це ваша цільова змінна, яку модель буде намагатися передбачити. Вона вже має бінарний формат, що ідеально підходить для логістичної регресії.\n",
        "\n",
        "5.  **Розбиття даних:**\n",
        "    * Завжди розбивайте дані на навчальний (training) та тестовий (testing) набори. Це дозволяє оцінити, наскільки добре ваша модель узагальнює дані, які вона \"ніколи не бачила\". Стандартне співвідношення 70/30 або 80/20.\n",
        "\n",
        "---\n",
        "\n",
        "## Параметри для кодування та масштабування ознак\n",
        "\n",
        "### 1. Кодування категоріальних ознак\n",
        "\n",
        "* **`Geography` (Франція, Іспанія, Німеччина):**\n",
        "    * **Метод:** **One-Hot Encoding** (Даммі-змінні). Оскільки між країнами немає природного порядку (Франція не \"більша\" за Іспанію), One-Hot Encoding є найкращим вибором. Він створить нові бінарні стовпці (наприклад, `Geography_France`, `Geography_Spain`, `Geography_Germany`), де 1 означає належність до цієї країни, а 0 - ні.\n",
        "    * **Важливий аспект:** Уникайте \"даммі-пастки\" (dummy variable trap). При One-Hot Encoding з `N` категорій зазвичай створюють `N-1` нових стовпців. Наприклад, якщо у вас є `Geography_France`, `Geography_Spain`, то `Geography_Germany` буде зрозуміло, коли обидва перші стовпці дорівнюють 0. Це запобігає мультиколінеарності.\n",
        "\n",
        "* **`Gender` (Чоловік, Жінка):**\n",
        "    * **Метод:** Також **One-Hot Encoding**, або простий **бінарний кодер** (наприклад, `Male` = 1, `Female` = 0). Оскільки тут лише дві категорії, ці методи по суті однакові. Можна перетворити \"Чоловік\" на 1, \"Жінка\" на 0.\n",
        "\n",
        "### 2. Масштабування числових ознак\n",
        "\n",
        "Для логістичної регресії рекомендується використовувати **стандартизацію**.\n",
        "\n",
        "* **Метод:** **StandardScaler**. Цей метод масштабує ознаки так, щоб вони мали нульове середнє (`mean=0`) та одиничне стандартне відхилення (`std=1`).\n",
        "* **Ознаки для масштабування:**\n",
        "    * `Credit Score`\n",
        "    * `Age`\n",
        "    * `Tenure`\n",
        "    * `Balance`\n",
        "    * `NumOfProducts`\n",
        "    * `EstimatedSalary`\n",
        "* **Ознаки, які НЕ потребують масштабування:**\n",
        "    * `HasCrCard` (вже бінарна, 0 або 1)\n",
        "    * `IsActiveMember` (вже бінарна, 0 або 1)\n",
        "    * Новостворені даммі-змінні з `Geography` та `Gender`.\n",
        "\n",
        "**Важливо:** Завжди застосовуйте `StandardScaler.fit()` лише до навчальних даних, а потім використовуйте `StandardScaler.transform()` для обох - навчальних та тестових даних. Це запобігає витоку інформації з тестового набору в навчальний процес.\n",
        "\n",
        "---\n",
        "\n",
        "## Оптимальне налаштування моделі логістичної регресії\n",
        "\n",
        "Логістична регресія є досить простою, але ефективною моделлю. Її \"оптимальне\" налаштування часто включає кілька кроків.\n",
        "\n",
        "1.  **Вибір класу моделі:**\n",
        "    * Використовуйте `LogisticRegression` з бібліотеки `scikit-learn`.\n",
        "\n",
        "2.  **Регуляризація:**\n",
        "    * Логістична регресія в `scikit-learn` за замовчуванням застосовує **L2-регуляризацію** (`penalty='l2'`). Це допомагає запобігти перенавчанню моделі, зменшуючи величину коефіцієнтів.\n",
        "    * **Параметр `C`**: Це обернена сила регуляризації. Менші значення `C` означають сильнішу регуляризацію (більше \"штрафу\" за великі коефіцієнти), а більші значення `C` - слабшу. За замовчуванням `C=1.0`.\n",
        "    * **Налаштування `C`**: Ви можете спробувати різні значення `C` (наприклад, 0.01, 0.1, 1, 10, 100) за допомогою методів крос-валідації.\n",
        "\n",
        "3.  **Оптимізатор:**\n",
        "    * Параметр `solver` визначає алгоритм, який використовується для оптимізації моделі (знаходження оптимальних коефіцієнтів). Для більшості випадків, особливо для невеликих та середніх датасетів, добре підходять:\n",
        "        * `'liblinear'` (добре для невеликих наборів даних, підтримує L1/L2 регуляризацію)\n",
        "        * `'lbfgs'` (хороший вибір за замовчуванням для багатьох задач, підтримує лише L2)\n",
        "        * `'saga'` (підходить для великих наборів даних та L1/L2/Elastic-Net регуляризації)\n",
        "    * Почніть з `'lbfgs'` або `'liblinear'`.\n",
        "\n",
        "4.  **Обробка незбалансованих класів (Class Imbalance):**\n",
        "    * **Увага!** Відтік клієнтів (коли `Exited=1`) часто є меншістю в наборі даних. Якщо відтоку набагато менше, ніж тих, хто залишився (наприклад, 10% проти 90%), модель може навчитися просто передбачати \"0\" для всіх, досягаючи високої точності, але будучи марною.\n",
        "    * **Рішення:**\n",
        "        * **`class_weight='balanced'`**: Це найпростіший і часто дуже ефективний спосіб. Він автоматично коригує ваги класів у функції втрат пропорційно частоті їх появи у навчальних даних. Це дає більший \"штраф\" за неправильну класифікацію міноритарного класу.\n",
        "        * **Передискретизація (Resampling):** Методи, такі як Oversampling міноритарного класу (наприклад, SMOTE) або Undersampling мажоритарного класу. Це складніший, але потужний підхід.\n",
        "\n",
        "5.  **Оцінка моделі:**\n",
        "    * Оскільки класи можуть бути незбалансованими, **accuracy (точність)** не є найкращою метрикою.\n",
        "    * Використовуйте метрики, які краще підходять для незбалансованих даних:\n",
        "        * **Precision, Recall, F1-Score**: Особливо F1-Score, який є гарним балансом між Precision та Recall.\n",
        "        * **ROC AUC (Receiver Operating Characteristic - Area Under the Curve)**: Ця метрика показує, наскільки добре модель розрізняє класи, незалежно від порогу класифікації. Вищий ROC AUC (ближче до 1) означає кращу модель.\n",
        "        * **Confusion Matrix**: Допомагає візуалізувати кількість правильних/неправильних класифікацій для кожного класу.\n",
        "\n",
        "6.  **Крос-валідація (Cross-Validation):**\n",
        "    * Завжди використовуйте крос-валідацію (наприклад, `KFold` або `StratifiedKFold` для незбалансованих класів) для більш надійної оцінки продуктивності моделі та для підбору гіперпараметрів (`C`, `solver`).\n",
        "    * `GridSearchCV` або `RandomizedSearchCV` можуть автоматизувати цей процес.\n",
        "\n",
        "### Приклад послідовності кроків:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Припустимо, у вас є DataFrame 'df' з вашими даними\n",
        "\n",
        "# 1. Створення фіктивних даних для прикладу (замініть на ваші реальні дані)\n",
        "data = {\n",
        "    'ID': range(100),\n",
        "    'Customer ID': [i % 50 for i in range(100)],\n",
        "    'Surname': [f'Surname_{i}' for i in range(100)],\n",
        "    'Credit Score': np.random.randint(350, 850, 100),\n",
        "    'Geography': np.random.choice(['France', 'Spain', 'Germany'], 100),\n",
        "    'Gender': np.random.choice(['Male', 'Female'], 100),\n",
        "    'Age': np.random.randint(18, 92, 100),\n",
        "    'Tenure': np.random.randint(0, 10, 100),\n",
        "    'Balance': np.random.uniform(0, 250000, 100),\n",
        "    'NumOfProducts': np.random.randint(1, 4, 100),\n",
        "    'HasCrCard': np.random.randint(0, 2, 100),\n",
        "    'IsActiveMember': np.random.randint(0, 2, 100),\n",
        "    'EstimatedSalary': np.random.uniform(100, 200000, 100),\n",
        "    'Exited': np.random.randint(0, 2, 100) # Цільова змінна\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Імітуємо незбалансованість: зробимо більшість Exited = 0\n",
        "df['Exited'] = np.random.choice([0, 1], size=100, p=[0.85, 0.15])\n",
        "\n",
        "\n",
        "# 2. Видалення непотрібних стовпців\n",
        "df = df.drop(['ID', 'Customer ID', 'Surname'], axis=1)\n",
        "\n",
        "# 3. Розбиття на ознаки (X) та цільову змінну (y)\n",
        "X = df.drop('Exited', axis=1)\n",
        "y = df['Exited']\n",
        "\n",
        "# 4. Визначення категоріальних та числових ознак\n",
        "categorical_features = ['Geography', 'Gender']\n",
        "numerical_features = [\n",
        "    'Credit Score', 'Age', 'Tenure', 'Balance',\n",
        "    'NumOfProducts', 'EstimatedSalary' # HasCrCard та IsActiveMember вже бінарні, не масштабуємо\n",
        "]\n",
        "\n",
        "# 5. Створення препроцесора за допомогою ColumnTransformer\n",
        "# One-Hot Encoding для категоріальних\n",
        "# StandardScaler для числових\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features) # drop='first' для уникнення даммі-пастки\n",
        "    ],\n",
        "    remainder='passthrough' # Залишити інші стовпці (HasCrCard, IsActiveMember) як є\n",
        ")\n",
        "\n",
        "# 6. Створення пайплайну: препроцесор + модель логістичної регресії\n",
        "# Використовуємо class_weight='balanced' для обробки незбалансованих класів\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# 7. Розбиття даних на навчальну та тестову вибірки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # stratify=y для збереження співвідношення класів\n",
        "\n",
        "# 8. Налаштування гіперпараметрів за допомогою GridSearchCV (або RandomizedSearchCV)\n",
        "# Визначаємо сітку параметрів для пошуку\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100], # Сила регуляризації\n",
        "    'classifier__solver': ['liblinear', 'lbfgs'] # Оптимізатор\n",
        "}\n",
        "\n",
        "# Використовуємо GridSearchCV для пошуку найкращих параметрів\n",
        "# scoring='roc_auc' - хороша метрика для незбалансованих класів\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(f\"\\nНайкращі параметри: {grid_search.best_params_}\")\n",
        "print(f\"Найкращий ROC AUC на крос-валідації: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# 9. Оцінка моделі на тестовій вибірці\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1] # Ймовірність для класу 1\n",
        "\n",
        "print(\"\\nЗвіт по класифікації на тестових даних:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(f\"ROC AUC на тестових даних: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "# Виведемо Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "```\n",
        "\n",
        "Цей код забезпечує повний цикл від препроцесингу до налаштування та оцінки моделі логістичної регресії, враховуючи ключові аспекти, які я описав. Не забудьте замінити фіктивні дані `df` на ваші реальні дані!\n",
        "\n",
        "Успіхів з вашим проектом!"
      ],
      "metadata": {
        "id": "qOM-QR-r-uls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "33ltiSRZBKbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "b3959737-62d3-4a3c-da3d-abc8de7cdf6f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2155457923.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneHotEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnTransformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HTMLDocumentationLinkMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvalidate_parameter_constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_routing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_bunch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBunch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_chunking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgen_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_estimator_html_repr\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mestimator_html_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_chunking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_param_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInterval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_is_arraylike_not_scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataConversionWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPositiveSpectrumWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_array_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_is_numpy_namespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeprecation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_deprecate_force_all_finite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_preserve_dia_indices_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0m_NUMPY_NAMESPACE_NAMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"numpy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"array_api_compat.numpy\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mpd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport] # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m from pandas.core.api import (\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0;31m# dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mArrowDtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m from pandas.core.algorithms import (\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mfactorize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0munique\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m )\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_algos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtake_nd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m from pandas.core.construction import (\n\u001b[1;32m     79\u001b[0m     \u001b[0marray\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd_array\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/Alenushka2013/Coursera/refs/heads/master/Datasets/bank-customer-churn-prediction.csv')"
      ],
      "metadata": {
        "id": "6jFZiQK8BKYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "Z1udjjUxBKWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 3:]"
      ],
      "metadata": {
        "id": "Ev-EC7nEBipA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Розбиття на ознаки (X) та цільову змінну (y)\n",
        "X = df.iloc[:, 3:].drop('Exited', axis=1)\n",
        "y = df['Exited']\n"
      ],
      "metadata": {
        "id": "gHWOIYueBKT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape, y.shape"
      ],
      "metadata": {
        "id": "2hxFIwgtCBpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "KF9gvPuRCF03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. Визначення категоріальних та числових ознак\n",
        "# Ми можемо динамічно визначити ці списки, щоб уникнути помилок через опечатки або зміни в даних\n",
        "all_features = X.columns.tolist() # Отримуємо всі колонки X\n",
        "\n",
        "numerical_features = [\n",
        "    col for col in all_features if X[col].dtype in ['int64', 'float64'] and col not in ['HasCrCard', 'IsActiveMember']\n",
        "]\n",
        "categorical_features = [\n",
        "    col for col in all_features if X[col].dtype == 'object'\n",
        "]\n",
        "# Бінарні ознаки, які вже є числовими (0 або 1)\n",
        "binary_features = ['HasCrCard', 'IsActiveMember']\n",
        "\n",
        "# 5. Створення препроцесора за допомогою ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', MinMaxScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first'), categorical_features),\n",
        "        ('bin', 'passthrough', binary_features) # Просто пропускаємо бінарні ознаки\n",
        "    ],\n",
        "    # remainder='drop' - якщо є інші колонки, які не вказані, видалити їх\n",
        "    # remainder='passthrough' - залишити інші колонки без змін (не в цьому випадку, бо всі колонки вказані)\n",
        ")\n",
        "\n",
        "# 6. Створення пайплайну: препроцесор + модель логістичної регресії\n",
        "# Використовуємо class_weight='balanced' для обробки незбалансованих класів\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced'))\n",
        "])\n",
        "\n",
        "# 7. Розбиття даних на навчальну та тестову вибірки\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # stratify=y для збереження співвідношення класів\n"
      ],
      "metadata": {
        "id": "EuEV6KlCB_0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Налаштування гіперпараметрів за допомогою GridSearchCV (або RandomizedSearchCV)\n",
        "# Визначаємо сітку параметрів для пошуку\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100], # Сила регуляризації (обернена)\n",
        "    'classifier__solver': ['liblinear', 'lbfgs'] # Оптимізатор\n",
        "}\n",
        "\n",
        "# Використовуємо GridSearchCV для пошуку найкращих параметрів\n",
        "# scoring='roc_auc' - хороша метрика для незбалансованих класів\n",
        "print(\"Запускаємо GridSearchCV для пошуку найкращих параметрів...\")\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train) # Тепер цей рядок має спрацювати коректно\n",
        "\n",
        "print(f\"\\nНайкращі параметри: {grid_search.best_params_}\")\n",
        "print(f\"Найкращий ROC AUC на крос-валідації: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "Ew4yRtEiCSWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Оцінка моделі на тестовій вибірці\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "y_pred_proba = best_model.predict_proba(X_test)[:, 1] # Ймовірність для класу 1\n",
        "\n",
        "# Виведемо Confusion Matrix\n",
        "print('Матриця на тренувальних даних')\n",
        "y_pred_train = best_model.predict(X_train)\n",
        "cm = confusion_matrix(y_pred_train, y_train, normalize='true').round(2)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "\n",
        "print(\"\\nЗвіт по класифікації на тестових даних:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "print(f\"ROC AUC на тестових даних: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n",
        "\n",
        "# Виведемо Confusion Matrix\n",
        "cm = confusion_matrix(y_pred, y_test, normalize='true').round(2)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)"
      ],
      "metadata": {
        "id": "G21titkiCc1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETbm2QU1BKRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-80ygbEpBKPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nN65AGEeBKMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AjrOixeyBKJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6pGWlt0lBJ5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHCpqfmBTg4h"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# определение модели\n",
        "model = XGBClassifier()"
      ],
      "metadata": {
        "id": "QGg4bsKT5tI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# определение наборов данных для оценки итераций\n",
        "evalset = [(X_train, y_train), (X_test, y_test)]"
      ],
      "metadata": {
        "id": "2V6XEYV86B8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#################\n",
        "model = XGBClassifier(\n",
        "    learning_rate=0.02,\n",
        "    n_estimators=10,\n",
        "    objective=\"binary:logistic\",\n",
        "    nthread=3,\n",
        "    tree_method=\"gpu_hist\"  # благодаря этому задействуется GPU.\n",
        ")\n",
        "\n",
        "import time\n",
        "print('Lets GO!')\n",
        "start = time.ctime()\n",
        "# обучение модели\n",
        "model.fit(X_train, y_train, eval_metric='logloss', eval_set=evalset)\n",
        "\n",
        "end = time.ctime()\n",
        "print('all done!')\n",
        "print('started', start)\n",
        "print('finished', end)"
      ],
      "metadata": {
        "id": "_cudMoAK5zFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# submission_log_reg_3   score   **0,93246**"
      ],
      "metadata": {
        "id": "RO0ggZ_iziIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, classification_report, root_mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib"
      ],
      "metadata": {
        "id": "fby67gvhzu4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.read_csv('https://raw.githubusercontent.com/Alenushka2013/Coursera/refs/heads/master/Datasets/bank-customer-churn-prediction.csv', index_col=0)\n",
        "\n",
        "train_df, val_df = train_test_split(raw_df, test_size=0.2, random_state=42, stratify=raw_df['Exited'])\n",
        "\n",
        "# Створюємо трен. і вал. набори\n",
        "input_cols = list(train_df.columns)[2:-1]\n",
        "target_col = 'Exited'\n",
        "train_inputs, train_targets = train_df[input_cols], train_df[target_col]\n",
        "val_inputs, val_targets = val_df[input_cols], val_df[target_col]\n",
        "\n",
        "# Виявляємо числові і категоріальні колонки\n",
        "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()[:-1]\n",
        "categorical_cols = train_inputs.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Створюємо трансформери для числових і категоріальних колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Комбінуємо трансформери для різних типів колонок в один препроцесор\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Стоврюємо пайплайн, який спочатку запускає препроцесинг, потім тренуєм модель\n",
        "model_pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Тренуємо пайплайн\n",
        "model_pipeline.fit(train_inputs, train_targets)\n",
        "\n",
        "# Функція, щоб передбачати і рахувати метрики\n",
        "def predict_and_plot(model_pipeline, inputs, targets, name=''):\n",
        "    preds = model_pipeline.predict(inputs)\n",
        "    roc_auc = roc_auc_score(preds, targets)\n",
        "    print(f\"Area under ROC score on {name} dataset: {roc_auc:.3f}\")\n",
        "    confusion_matrix_ = confusion_matrix(targets, preds, normalize='true')\n",
        "    plt.figure()\n",
        "    sns.heatmap(confusion_matrix_, annot=True, cmap='Blues')\n",
        "    plt.xlabel('Prediction')\n",
        "    plt.ylabel('Target')\n",
        "    plt.title('{} Confusion Matrix'.format(name))\n",
        "    plt.show()\n",
        "    return preds\n",
        "\n",
        "# Оцінюємо модель на трен і вал даних\n",
        "train_preds = predict_and_plot(model_pipeline, train_inputs, train_targets, 'Training')\n",
        "val_preds = predict_and_plot(model_pipeline, val_inputs, val_targets, 'Validation')\n",
        "\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(val_targets, model_pipeline.predict(val_inputs)))\n",
        "\n",
        "# Зберігаємо модель для подальшого використання\n",
        "joblib.dump(model_pipeline, 'bank-customer_pipeline.joblib')"
      ],
      "metadata": {
        "id": "S-_BeIAxzzrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "?LogisticRegression"
      ],
      "metadata": {
        "id": "qIK0o-6OTwMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Створюємо трансформери для числових колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('polynomial_features', PolynomialFeatures(degree=5))\n",
        "])\n",
        "\n",
        "# Комбінуємо трансформери для різних типів колонок в один препроцесор\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Стоврюємо пайплайн, який спочатку запускає препроцесинг, потім тренуєм модель\n",
        "model_pipeline_pol5 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='liblinear'))\n",
        "])\n",
        "\n",
        "# Тренуємо пайплайн\n",
        "model_pipeline_pol5.fit(train_inputs, train_targets)\n",
        "\n",
        "# Оцінюємо модель на трен і вал даних\n",
        "train_preds = predict_and_plot(model_pipeline_pol5, train_inputs, train_targets, 'Training')\n",
        "val_preds = predict_and_plot(model_pipeline_pol5, val_inputs, val_targets, 'Validation')\n",
        "\n",
        "# # Звіт про класифікацію\n",
        "# print(classification_report(val_targets, model_pipeline_pol5.predict(val_inputs)))\n",
        "\n",
        "# Assuming val_targets are your true labels and model_pipeline.predict(val_inputs) are your predictions\n",
        "report = classification_report(val_targets, model_pipeline_pol5.predict(val_inputs), output_dict=True)\n",
        "df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Printing with 3 decimal places\n",
        "print(df.round(3))\n",
        "\n",
        "# Зберігаємо модель для подальшого використання\n",
        "joblib.dump(model_pipeline_pol5, 'bank-customer_pipeline_pol5.joblib')"
      ],
      "metadata": {
        "id": "9MXT-y1xz9S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subm_df  = pd.read_csv('/content/sample_submission.csv')\n",
        "\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "test_pred = model_pipeline_pol5.predict_proba(test_df)[:, 1]\n",
        "\n",
        "subm_df['Exited'] = model_pipeline_pol5.predict_proba(test_df)[:, 1]"
      ],
      "metadata": {
        "id": "KAUbzeRk0A3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subm_df.head()"
      ],
      "metadata": {
        "id": "qsGmsJ5w0GPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# submission_log_reg_4 score 0.93270"
      ],
      "metadata": {
        "id": "kpYdaoTZZmj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Створюємо трансформери для числових колонок\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('polynomial_features', PolynomialFeatures(degree=5))\n",
        "])\n",
        "\n",
        "# Комбінуємо трансформери для різних типів колонок в один препроцесор\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Стоврюємо пайплайн, який спочатку запускає препроцесинг, потім тренуєм модель\n",
        "model_pipeline_pol5 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(solver='lbfgs', penalty = 'l2'))\n",
        "\n",
        "])\n",
        "\n",
        "# Тренуємо пайплайн\n",
        "model_pipeline_pol5.fit(train_inputs, train_targets)\n",
        "\n",
        "# Оцінюємо модель на трен і вал даних\n",
        "train_preds = predict_and_plot(model_pipeline_pol5, train_inputs, train_targets, 'Training')\n",
        "val_preds = predict_and_plot(model_pipeline_pol5, val_inputs, val_targets, 'Validation')\n",
        "\n",
        "# # Звіт про класифікацію\n",
        "# print(classification_report(val_targets, model_pipeline_pol5.predict(val_inputs)))\n",
        "\n",
        "# Assuming val_targets are your true labels and model_pipeline.predict(val_inputs) are your predictions\n",
        "report = classification_report(val_targets, model_pipeline_pol5.predict(val_inputs), output_dict=True)\n",
        "df = pd.DataFrame(report).transpose()\n",
        "\n",
        "# Printing with 3 decimal places\n",
        "print(df.round(3))\n",
        "\n",
        "# Зберігаємо модель для подальшого використання\n",
        "joblib.dump(model_pipeline_pol5, 'bank-customer_pipeline_pol5.joblib')"
      ],
      "metadata": {
        "id": "j5_wmZ_LZqUN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "subm_df  = pd.read_csv('/content/sample_submission.csv')\n",
        "\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "test_pred = model_pipeline_pol5.predict_proba(test_df)[:, 1]\n",
        "\n",
        "subm_df['Exited'] = model_pipeline_pol5.predict_proba(test_df)[:, 1]\n",
        "\n",
        "subm_df.to_csv('submission_log_reg_4.csv', index=False)"
      ],
      "metadata": {
        "id": "oiy81FcoZ3DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subm_df.head()"
      ],
      "metadata": {
        "id": "kHibgPSEaeFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Припустимо твій пайплайн називається model_pipeline_pol4\n",
        "preprocessor = model_pipeline_pol5.named_steps['preprocessor']\n",
        "\n",
        "# Після фіта протренованого пайплайну\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "print(len(feature_names))"
      ],
      "metadata": {
        "id": "D93w8yZTG62G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #coefs_df = pd.DataFrame(poly_features.get_feature_names_out(X_train.columns), columns=['feature_name'])\n",
        "# coefs_df['value'] = model_pipeline_pol5.coef_.round(5).flatten()\n",
        "# coefs_df.set_index('feature_name').sort_values(by='value', ascending=False).style.background_gradient()\n",
        "\n",
        "# Отримання трансформованих назв ознак\n",
        "transformed_feature_names = model_pipeline_pol5.named_steps['preprocessor'].get_feature_names_out()\n",
        "\n",
        "# Отримання коефіцієнтів з класифікатора\n",
        "coefficients = model_pipeline_pol5.named_steps['classifier'].coef_[0]\n",
        "intercept = model_pipeline_pol5.named_steps['classifier'].intercept_[0]\n",
        "\n",
        "# Створення DataFrame для зручного перегляду\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': transformed_feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Сортування за абсолютною величиною коефіцієнтів\n",
        "feature_importance_df['Abs_Coefficient'] = np.abs(feature_importance_df['Coefficient'])\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"\\n--- Найзначущіші ознаки (за абсолютною величиною коефіцієнта) ---\")\n",
        "print(feature_importance_df.head(20)) # Виведемо топ-20 найзначущіших\n",
        "\n",
        "print(f\"\\nІнтерсепт (зсув): {intercept:.4f}\")"
      ],
      "metadata": {
        "id": "z-2aLNKbGEs0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(feature_importance_df.tail(20))"
      ],
      "metadata": {
        "id": "lzGItrrJKffR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subm_df.to_csv('submission_log_reg_3.csv', index=False)"
      ],
      "metadata": {
        "id": "IMYu-Spp0JI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Варіант з балансуванням класів"
      ],
      "metadata": {
        "id": "b27Ye3czNSGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline as SklearnPipeline # Імпортуємо sklearn Pipeline для внутрішніх трансформерів\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, PolynomialFeatures\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, confusion_matrix, roc_auc_score, classification_report, root_mean_squared_error\n",
        "\n",
        "# Для ресемплінгу використовуємо спеціальний пайплайн від imblearn\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler, TomekLinks\n",
        "\n",
        "raw_df = pd.read_csv('https://raw.githubusercontent.com/Alenushka2013/Coursera/refs/heads/master/Datasets/bank-customer-churn-prediction.csv', index_col=0)\n"
      ],
      "metadata": {
        "id": "V5F1926g-nGt"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(raw_df, test_size=0.2, random_state=42, stratify=raw_df['Exited'])\n",
        "\n",
        "# Створюємо трен. і вал. набори\n",
        "input_cols = list(train_df.columns)[2:-1]\n",
        "target_col = 'Exited'\n",
        "train_inputs, train_targets = train_df[input_cols], train_df[target_col]\n",
        "val_inputs, val_targets = val_df[input_cols], val_df[target_col]\n",
        "\n",
        "# Виявляємо числові і категоріальні колонки\n",
        "numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = train_inputs.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# parameters\n",
        "random_state = 42\n",
        "solver='liblinear'\n",
        "penalty = 'l2'\n",
        "degree = 5\n",
        "\n",
        "# Створюємо трансформери для числових і категоріальних колонок\n",
        "numeric_transformer = SklearnPipeline(steps=[\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('polynomial_features', PolynomialFeatures(degree=degree, include_bias=False)) # include_bias=False для PCA\n",
        "])\n",
        "\n",
        "categorical_transformer = SklearnPipeline(steps=[\n",
        "    ('onehot', OneHotEncoder(sparse_output=False, handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Комбінуємо трансформери для різних типів колонок в один препроцесор\n",
        "# Це буде перший крок у нашому ImbPipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ],\n",
        "    remainder='drop'\n",
        ")\n",
        "\n",
        "# --- Інтеграція PCA ---\n",
        "# Ви можете визначити кількість компонентів PCA\n",
        "# Наприклад, n_components = 0.95 (зберегти 95% дисперсії) або n_components = 5 (конкретна кількість)\n",
        "# Важливо: PCA застосовується *після* препроцесора\n",
        "pca_transformer = PCA(n_components=0.90, random_state=42) # Або будь-яке інше значення\n",
        "\n",
        "# --- Створення RandomOverSampler та RandomUnderSampler ---\n",
        "oversampler = RandomOverSampler(random_state=random_state, sampling_strategy='auto')\n",
        "undersampler = RandomUnderSampler(random_state=random_state, sampling_strategy='auto')\n",
        "smote = SMOTE(random_state=random_state)\n",
        "adasyn = ADASYN(random_state=random_state)\n",
        "tomek = TomekLinks()\n",
        "\n",
        "# --- Варіант 1: Пайплайн з RandomOverSampler ---\n",
        "# Використовуємо imblearn.pipeline.Pipeline\n",
        "model_pipeline_with_oversampling = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),          # Крок 1: Препроцесинг даних\n",
        "    ('pca', pca_transformer),                # Крок 2: PCA (опціонально)\n",
        "    ('oversampler', oversampler),            # Крок 3: Ресемплінг (застосовується лише до X_train)\n",
        "    ('classifier', LogisticRegression(solver=solver, penalty = penalty, random_state=42, #max_iter=1000\n",
        "                                      )) # Крок 4: Класифікатор\n",
        "])\n",
        "\n",
        "print(\"Тренування пайплайну з Random Over-sampling...\")\n",
        "model_pipeline_with_oversampling.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "# --- Варіант 2: Пайплайн з RandomUnderSampler ---\n",
        "# Ви можете створити окремий пайплайн для кожного типу ресемплінгу\n",
        "model_pipeline_with_undersampling = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca_transformer),\n",
        "    ('undersampler', undersampler), # Змінюємо на RandomUnderSampler\n",
        "    ('classifier', LogisticRegression(solver=solver,  penalty = penalty,random_state=42, #max_iter=1000\n",
        "                                      ))\n",
        "])\n",
        "\n",
        "print(\"\\nТренування пайплайну з Random Under-sampling...\")\n",
        "model_pipeline_with_undersampling.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "# --- Варіант 3: Пайплайн без ресемплінгу (для порівняння) ---\n",
        "# Це звичайний sklearn.pipeline.Pipeline\n",
        "model_pipeline_no_sampling = SklearnPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca_transformer),\n",
        "    ('classifier', LogisticRegression(solver=solver,  penalty = penalty,random_state=42, #max_iter=1000\n",
        "                                      ))\n",
        "])\n",
        "\n",
        "print(\"\\nТренування пайплайну без ресемплінгу...\")\n",
        "model_pipeline_no_sampling.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "# --- Варіант 4: Пайплайн зі SMOTE (для порівняння) ---\n",
        "# Це звичайний sklearn.pipeline.Pipeline\n",
        "model_pipeline_with_SMOTE = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca_transformer),\n",
        "    ('smote', smote),\n",
        "    ('classifier', LogisticRegression(solver=solver,  penalty = penalty,random_state=42, #max_iter=1000\n",
        "                                      ))\n",
        "])\n",
        "\n",
        "print(\"\\nТренування пайплайну зі SMOTE...\")\n",
        "model_pipeline_with_SMOTE.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "# --- Варіант 5: Пайплайн з adasyn (для порівняння) ---\n",
        "# Це звичайний sklearn.pipeline.Pipeline\n",
        "model_pipeline_with_adasyn = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca_transformer),\n",
        "    ('adasyn', adasyn),\n",
        "    ('classifier', LogisticRegression(solver=solver,  penalty = penalty,random_state=42, #max_iter=1000\n",
        "                                      ))\n",
        "])\n",
        "\n",
        "print(\"\\nТренування пайплайну з adasyn...\")\n",
        "model_pipeline_with_adasyn.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "# --- Варіант 6: Пайплайн з tomek (для порівняння) ---\n",
        "# Це звичайний sklearn.pipeline.Pipeline\n",
        "model_pipeline_with_tomek = ImbPipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('pca', pca_transformer),\n",
        "    ('tomek', tomek),\n",
        "    ('classifier', LogisticRegression(solver=solver,  penalty = penalty,random_state=42, #max_iter=1000\n",
        "                                      ))\n",
        "])\n",
        "\n",
        "print(\"\\nТренування пайплайну з tomek...\")\n",
        "model_pipeline_with_tomek.fit(train_inputs, train_targets)\n",
        "print(\"Тренування завершено.\")\n",
        "\n",
        "#Тепер ви можете оцінювати ці пайплайни на тестових даних:\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(raw_df[numeric_cols + categorical_cols], raw_df['Exited'], test_size=0.2, random_state=42, stratify=raw_df['Exited'])\n",
        "\n",
        "y_pred_oversampling = model_pipeline_with_oversampling.predict(X_test)\n",
        "y_pred_proba_oversampling = model_pipeline_with_oversampling.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (Oversampling): {roc_auc_score(y_test, y_pred_proba_oversampling):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_with_oversampling.predict(X_test)))\n",
        "\n",
        "y_pred_undersampling = model_pipeline_with_undersampling.predict(X_test)\n",
        "y_pred_proba_undersampling = model_pipeline_with_undersampling.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (undersampling): {roc_auc_score(y_test, y_pred_proba_undersampling):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_with_undersampling.predict(X_test)))\n",
        "\n",
        "y_pred_no_sampling = model_pipeline_no_sampling.predict(X_test)\n",
        "y_pred_proba_no_sampling = model_pipeline_no_sampling.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (no_sampling): {roc_auc_score(y_test, y_pred_proba_no_sampling):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_no_sampling.predict(X_test)))\n",
        "\n",
        "y_pred_with_SMOTE = model_pipeline_with_SMOTE.predict(X_test)\n",
        "y_pred_proba_with_SMOTE = model_pipeline_with_SMOTE.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (with_SMOTE): {roc_auc_score(y_test, y_pred_proba_with_SMOTE):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_with_SMOTE.predict(X_test)))\n",
        "\n",
        "y_pred_with_adasyn = model_pipeline_with_adasyn.predict(X_test)\n",
        "y_pred_proba_with_adasyn = model_pipeline_with_adasyn.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (with_adasyn): {roc_auc_score(y_test, y_pred_proba_with_adasyn):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_with_adasyn.predict(X_test)))\n",
        "\n",
        "y_pred_with_tomek = model_pipeline_with_tomek.predict(X_test)\n",
        "y_pred_proba_with_tomek = model_pipeline_with_tomek.predict_proba(X_test)[:, 1]\n",
        "print(f\"\\nROC AUC (with_tomek): {roc_auc_score(y_test, y_pred_proba_with_tomek):.4f}\")\n",
        "# Звіт про класифікацію\n",
        "print(classification_report(y_test, model_pipeline_with_tomek.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIuSU7WUB_ZG",
        "outputId": "69720753-30f7-43d0-9aa8-4b9f78036f32"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Тренування пайплайну з Random Over-sampling...\n",
            "Тренування завершено.\n",
            "\n",
            "Тренування пайплайну з Random Under-sampling...\n",
            "Тренування завершено.\n",
            "\n",
            "Тренування пайплайну без ресемплінгу...\n",
            "Тренування завершено.\n",
            "\n",
            "Тренування пайплайну зі SMOTE...\n",
            "Тренування завершено.\n",
            "\n",
            "Тренування пайплайну з adasyn...\n",
            "Тренування завершено.\n",
            "\n",
            "Тренування пайплайну з tomek...\n",
            "Тренування завершено.\n",
            "\n",
            "ROC AUC (Oversampling): 0.8540\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.77      0.85      2390\n",
            "         1.0       0.48      0.81      0.60       610\n",
            "\n",
            "    accuracy                           0.78      3000\n",
            "   macro avg       0.71      0.79      0.72      3000\n",
            "weighted avg       0.85      0.78      0.80      3000\n",
            "\n",
            "\n",
            "ROC AUC (undersampling): 0.8539\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.77      0.85      2390\n",
            "         1.0       0.48      0.81      0.60       610\n",
            "\n",
            "    accuracy                           0.78      3000\n",
            "   macro avg       0.71      0.79      0.72      3000\n",
            "weighted avg       0.85      0.78      0.80      3000\n",
            "\n",
            "\n",
            "ROC AUC (no_sampling): 0.8484\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.86      0.95      0.90      2390\n",
            "         1.0       0.68      0.38      0.48       610\n",
            "\n",
            "    accuracy                           0.84      3000\n",
            "   macro avg       0.77      0.67      0.69      3000\n",
            "weighted avg       0.82      0.84      0.82      3000\n",
            "\n",
            "\n",
            "ROC AUC (with_SMOTE): 0.8555\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.94      0.77      0.85      2390\n",
            "         1.0       0.48      0.81      0.60       610\n",
            "\n",
            "    accuracy                           0.78      3000\n",
            "   macro avg       0.71      0.79      0.73      3000\n",
            "weighted avg       0.85      0.78      0.80      3000\n",
            "\n",
            "\n",
            "ROC AUC (with_adasyn): 0.8566\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.95      0.74      0.83      2390\n",
            "         1.0       0.45      0.84      0.59       610\n",
            "\n",
            "    accuracy                           0.76      3000\n",
            "   macro avg       0.70      0.79      0.71      3000\n",
            "weighted avg       0.85      0.76      0.78      3000\n",
            "\n",
            "\n",
            "ROC AUC (with_tomek): 0.8497\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.87      0.94      0.90      2390\n",
            "         1.0       0.67      0.44      0.53       610\n",
            "\n",
            "    accuracy                           0.84      3000\n",
            "   macro avg       0.77      0.69      0.72      3000\n",
            "weighted avg       0.83      0.84      0.83      3000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model_pipeline_with_oversampling.predict_proba(X_test)[:, 1]"
      ],
      "metadata": {
        "id": "Kq5Y1FjK4DB3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}